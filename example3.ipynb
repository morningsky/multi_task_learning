{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "890a3aa13989506a4f157c210d7e888dcd9c9bfc264e152253245030dde69647"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat,get_feature_names\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### task 1 aims to predict whether the education level is at least college;\n",
    "### task 2 aims to predict whether this personâ€™s marital status is never married."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['age', 'class_worker', 'det_ind_code', 'det_occ_code', 'education', 'wage_per_hour', 'hs_college',\n",
    "                'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member',\n",
    "                'unemp_reason', 'full_or_part_emp', 'capital_gains', 'capital_losses', 'stock_dividends',\n",
    "                'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ',\n",
    "                'instance_weight', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',\n",
    "                'num_emp', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',\n",
    "                'own_or_self', 'vet_question', 'vet_benefits', 'weeks_worked', 'year', 'income_50k']\n",
    "df_train = pd.read_csv('./data/census-income/census-income.data',header=None,names=column_names)\n",
    "df_test = pd.read_csv('./data/census-income/census-income.test',header=None,names=column_names)\n",
    "data = pd.concat([df_train, df_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       " High school graduate                      72554\n",
       " Children                                  70864\n",
       " Some college but no degree                41774\n",
       " Bachelors degree(BA AB BS)                29750\n",
       " 7th and 8th grade                         12156\n",
       " 10th grade                                11370\n",
       " 11th grade                                10399\n",
       " Masters degree(MA MS MEng MEd MSW MBA)     9847\n",
       " 9th grade                                  9335\n",
       " Associates degree-occup /vocational        8048\n",
       " Associates degree-academic program         6442\n",
       " 5th or 6th grade                           4991\n",
       " 12th grade no diploma                      3263\n",
       " 1st 2nd 3rd or 4th grade                   2705\n",
       " Prof school degree (MD DDS DVM LLB JD)     2669\n",
       " Doctorate degree(PhD EdD)                  1883\n",
       " Less than 1st grade                        1235\n",
       "Name: education, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#task 1 label 'education' predict whether the education level is at least college\n",
    "data['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       " Never married                      129628\n",
       " Married-civilian spouse present    126315\n",
       " Divorced                            19160\n",
       " Widowed                             15788\n",
       " Separated                            5156\n",
       " Married-spouse absent                2234\n",
       " Married-A F spouse present           1004\n",
       "Name: marital_stat, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "#task 2 label: 'marital_stat'\n",
    "data['marital_stat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the label to binary classification\n",
    "college = [' Some college but no degree', ' Bachelors degree(BA AB BS)', ' Masters degree(MA MS MEng MEd MSW MBA)', ' Prof school degree (MD DDS DVM LLB JD)', ' Doctorate degree(PhD EdD)']\n",
    "\n",
    "data['label_education'] = data['education'].apply(lambda x: 1 if x in college else 0)\n",
    "data['label_marital'] = data['marital_stat'].apply(lambda x: 1 if x==' Never married' else 0)\n",
    "data.drop(labels=['education', 'marital_stat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dense and sparse features. \n",
    "#the functions used here can reference https://deepctr-torch.readthedocs.io/en/latest/Quick-Start.html\n",
    "columns = data.columns.values.tolist()\n",
    "sparse_features = ['class_worker', 'det_ind_code', 'det_occ_code', 'hs_college', 'major_ind_code',\n",
    "                        'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason',\n",
    "                        'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat',\n",
    "                        'det_hh_summ', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',\n",
    "                        'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',\n",
    "                        'vet_question', 'income_50k']\n",
    "dense_features = [col for col in columns if col not in sparse_features and col not in ['label_education', 'label_marital']]\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "    \n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "    \n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].max()+1, embedding_dim=4)for feat in sparse_features] \\\n",
    "                        + [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test dataset into 1:1 validation to test according to the MMOE paper\n",
    "# validation_split = n_val/len(train) = 0.2\n",
    "n_train = df_train.shape[0]\n",
    "n_val = df_test.shape[0]//2\n",
    "train = data[:n_train+n_val]\n",
    "test = data[n_train+n_val:]\n",
    "\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "195/195 - 2s - loss: 0.5787 - label_education_loss: 0.4041 - label_marital_loss: 0.1742 - label_education_auc: 0.8528 - label_marital_auc_1: 0.9797 - val_loss: 0.4874 - val_label_education_loss: 0.3896 - val_label_marital_loss: 0.0971 - val_label_education_auc: 0.8680 - val_label_marital_auc_1: 0.9939\n",
      "Epoch 2/100\n",
      "195/195 - 1s - loss: 0.4797 - label_education_loss: 0.3828 - label_marital_loss: 0.0962 - label_education_auc: 0.8717 - label_marital_auc_1: 0.9939 - val_loss: 0.4835 - val_label_education_loss: 0.3849 - val_label_marital_loss: 0.0978 - val_label_education_auc: 0.8692 - val_label_marital_auc_1: 0.9939\n",
      "Epoch 3/100\n",
      "195/195 - 1s - loss: 0.4771 - label_education_loss: 0.3818 - label_marital_loss: 0.0944 - label_education_auc: 0.8724 - label_marital_auc_1: 0.9941 - val_loss: 0.4817 - val_label_education_loss: 0.3870 - val_label_marital_loss: 0.0938 - val_label_education_auc: 0.8703 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 4/100\n",
      "195/195 - 1s - loss: 0.4737 - label_education_loss: 0.3798 - label_marital_loss: 0.0930 - label_education_auc: 0.8741 - label_marital_auc_1: 0.9942 - val_loss: 0.4793 - val_label_education_loss: 0.3853 - val_label_marital_loss: 0.0930 - val_label_education_auc: 0.8708 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 5/100\n",
      "195/195 - 1s - loss: 0.4725 - label_education_loss: 0.3788 - label_marital_loss: 0.0926 - label_education_auc: 0.8747 - label_marital_auc_1: 0.9942 - val_loss: 0.4778 - val_label_education_loss: 0.3834 - val_label_marital_loss: 0.0934 - val_label_education_auc: 0.8713 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 6/100\n",
      "195/195 - 1s - loss: 0.4717 - label_education_loss: 0.3784 - label_marital_loss: 0.0921 - label_education_auc: 0.8751 - label_marital_auc_1: 0.9943 - val_loss: 0.4784 - val_label_education_loss: 0.3830 - val_label_marital_loss: 0.0943 - val_label_education_auc: 0.8710 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 7/100\n",
      "195/195 - 1s - loss: 0.4698 - label_education_loss: 0.3776 - label_marital_loss: 0.0911 - label_education_auc: 0.8758 - label_marital_auc_1: 0.9944 - val_loss: 0.4829 - val_label_education_loss: 0.3824 - val_label_marital_loss: 0.0994 - val_label_education_auc: 0.8713 - val_label_marital_auc_1: 0.9941\n",
      "Epoch 8/100\n",
      "195/195 - 1s - loss: 0.4692 - label_education_loss: 0.3772 - label_marital_loss: 0.0908 - label_education_auc: 0.8761 - label_marital_auc_1: 0.9945 - val_loss: 0.4754 - val_label_education_loss: 0.3821 - val_label_marital_loss: 0.0921 - val_label_education_auc: 0.8718 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 9/100\n",
      "195/195 - 1s - loss: 0.4686 - label_education_loss: 0.3771 - label_marital_loss: 0.0903 - label_education_auc: 0.8761 - label_marital_auc_1: 0.9945 - val_loss: 0.4777 - val_label_education_loss: 0.3835 - val_label_marital_loss: 0.0930 - val_label_education_auc: 0.8709 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 10/100\n",
      "195/195 - 1s - loss: 0.4675 - label_education_loss: 0.3762 - label_marital_loss: 0.0899 - label_education_auc: 0.8769 - label_marital_auc_1: 0.9946 - val_loss: 0.4776 - val_label_education_loss: 0.3839 - val_label_marital_loss: 0.0924 - val_label_education_auc: 0.8718 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 11/100\n",
      "195/195 - 1s - loss: 0.4675 - label_education_loss: 0.3764 - label_marital_loss: 0.0898 - label_education_auc: 0.8767 - label_marital_auc_1: 0.9946 - val_loss: 0.4761 - val_label_education_loss: 0.3828 - val_label_marital_loss: 0.0919 - val_label_education_auc: 0.8715 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 12/100\n",
      "195/195 - 1s - loss: 0.4671 - label_education_loss: 0.3758 - label_marital_loss: 0.0899 - label_education_auc: 0.8773 - label_marital_auc_1: 0.9946 - val_loss: 0.4757 - val_label_education_loss: 0.3825 - val_label_marital_loss: 0.0919 - val_label_education_auc: 0.8723 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 13/100\n",
      "195/195 - 1s - loss: 0.4659 - label_education_loss: 0.3754 - label_marital_loss: 0.0891 - label_education_auc: 0.8777 - label_marital_auc_1: 0.9946 - val_loss: 0.4767 - val_label_education_loss: 0.3837 - val_label_marital_loss: 0.0916 - val_label_education_auc: 0.8722 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 14/100\n",
      "195/195 - 1s - loss: 0.4661 - label_education_loss: 0.3752 - label_marital_loss: 0.0895 - label_education_auc: 0.8776 - label_marital_auc_1: 0.9946 - val_loss: 0.4734 - val_label_education_loss: 0.3804 - val_label_marital_loss: 0.0916 - val_label_education_auc: 0.8730 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 15/100\n",
      "195/195 - 1s - loss: 0.4661 - label_education_loss: 0.3755 - label_marital_loss: 0.0892 - label_education_auc: 0.8775 - label_marital_auc_1: 0.9946 - val_loss: 0.4752 - val_label_education_loss: 0.3819 - val_label_marital_loss: 0.0919 - val_label_education_auc: 0.8726 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 16/100\n",
      "195/195 - 1s - loss: 0.4652 - label_education_loss: 0.3751 - label_marital_loss: 0.0886 - label_education_auc: 0.8778 - label_marital_auc_1: 0.9947 - val_loss: 0.4784 - val_label_education_loss: 0.3834 - val_label_marital_loss: 0.0936 - val_label_education_auc: 0.8717 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 17/100\n",
      "195/195 - 1s - loss: 0.4648 - label_education_loss: 0.3749 - label_marital_loss: 0.0885 - label_education_auc: 0.8779 - label_marital_auc_1: 0.9947 - val_loss: 0.4761 - val_label_education_loss: 0.3824 - val_label_marital_loss: 0.0922 - val_label_education_auc: 0.8724 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 18/100\n",
      "195/195 - 1s - loss: 0.4651 - label_education_loss: 0.3750 - label_marital_loss: 0.0886 - label_education_auc: 0.8779 - label_marital_auc_1: 0.9947 - val_loss: 0.4747 - val_label_education_loss: 0.3815 - val_label_marital_loss: 0.0917 - val_label_education_auc: 0.8734 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 19/100\n",
      "195/195 - 1s - loss: 0.4645 - label_education_loss: 0.3747 - label_marital_loss: 0.0883 - label_education_auc: 0.8781 - label_marital_auc_1: 0.9947 - val_loss: 0.4744 - val_label_education_loss: 0.3813 - val_label_marital_loss: 0.0915 - val_label_education_auc: 0.8731 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 20/100\n",
      "195/195 - 1s - loss: 0.4643 - label_education_loss: 0.3742 - label_marital_loss: 0.0885 - label_education_auc: 0.8785 - label_marital_auc_1: 0.9947 - val_loss: 0.4728 - val_label_education_loss: 0.3805 - val_label_marital_loss: 0.0907 - val_label_education_auc: 0.8735 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 21/100\n",
      "195/195 - 1s - loss: 0.4633 - label_education_loss: 0.3737 - label_marital_loss: 0.0881 - label_education_auc: 0.8790 - label_marital_auc_1: 0.9948 - val_loss: 0.4736 - val_label_education_loss: 0.3807 - val_label_marital_loss: 0.0913 - val_label_education_auc: 0.8729 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 22/100\n",
      "195/195 - 1s - loss: 0.4627 - label_education_loss: 0.3733 - label_marital_loss: 0.0878 - label_education_auc: 0.8792 - label_marital_auc_1: 0.9948 - val_loss: 0.4743 - val_label_education_loss: 0.3814 - val_label_marital_loss: 0.0913 - val_label_education_auc: 0.8738 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 23/100\n",
      "195/195 - 1s - loss: 0.4631 - label_education_loss: 0.3733 - label_marital_loss: 0.0882 - label_education_auc: 0.8792 - label_marital_auc_1: 0.9948 - val_loss: 0.4727 - val_label_education_loss: 0.3795 - val_label_marital_loss: 0.0916 - val_label_education_auc: 0.8739 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 24/100\n",
      "195/195 - 1s - loss: 0.4620 - label_education_loss: 0.3729 - label_marital_loss: 0.0875 - label_education_auc: 0.8795 - label_marital_auc_1: 0.9949 - val_loss: 0.4754 - val_label_education_loss: 0.3828 - val_label_marital_loss: 0.0909 - val_label_education_auc: 0.8718 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 25/100\n",
      "195/195 - 1s - loss: 0.4620 - label_education_loss: 0.3727 - label_marital_loss: 0.0877 - label_education_auc: 0.8797 - label_marital_auc_1: 0.9948 - val_loss: 0.4751 - val_label_education_loss: 0.3804 - val_label_marital_loss: 0.0931 - val_label_education_auc: 0.8734 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 26/100\n",
      "195/195 - 1s - loss: 0.4618 - label_education_loss: 0.3728 - label_marital_loss: 0.0874 - label_education_auc: 0.8795 - label_marital_auc_1: 0.9949 - val_loss: 0.4747 - val_label_education_loss: 0.3818 - val_label_marital_loss: 0.0913 - val_label_education_auc: 0.8734 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 27/100\n",
      "195/195 - 1s - loss: 0.4618 - label_education_loss: 0.3725 - label_marital_loss: 0.0876 - label_education_auc: 0.8798 - label_marital_auc_1: 0.9949 - val_loss: 0.4765 - val_label_education_loss: 0.3827 - val_label_marital_loss: 0.0921 - val_label_education_auc: 0.8742 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 28/100\n",
      "195/195 - 1s - loss: 0.4618 - label_education_loss: 0.3725 - label_marital_loss: 0.0875 - label_education_auc: 0.8797 - label_marital_auc_1: 0.9949 - val_loss: 0.4773 - val_label_education_loss: 0.3816 - val_label_marital_loss: 0.0939 - val_label_education_auc: 0.8735 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 29/100\n",
      "195/195 - 1s - loss: 0.4610 - label_education_loss: 0.3720 - label_marital_loss: 0.0873 - label_education_auc: 0.8802 - label_marital_auc_1: 0.9949 - val_loss: 0.4738 - val_label_education_loss: 0.3808 - val_label_marital_loss: 0.0913 - val_label_education_auc: 0.8736 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 30/100\n",
      "195/195 - 1s - loss: 0.4610 - label_education_loss: 0.3721 - label_marital_loss: 0.0871 - label_education_auc: 0.8802 - label_marital_auc_1: 0.9949 - val_loss: 0.4733 - val_label_education_loss: 0.3803 - val_label_marital_loss: 0.0913 - val_label_education_auc: 0.8738 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 31/100\n",
      "195/195 - 1s - loss: 0.4610 - label_education_loss: 0.3720 - label_marital_loss: 0.0873 - label_education_auc: 0.8803 - label_marital_auc_1: 0.9949 - val_loss: 0.4773 - val_label_education_loss: 0.3820 - val_label_marital_loss: 0.0935 - val_label_education_auc: 0.8717 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 32/100\n",
      "195/195 - 1s - loss: 0.4609 - label_education_loss: 0.3719 - label_marital_loss: 0.0872 - label_education_auc: 0.8803 - label_marital_auc_1: 0.9949 - val_loss: 0.4729 - val_label_education_loss: 0.3806 - val_label_marital_loss: 0.0905 - val_label_education_auc: 0.8729 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 33/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "195/195 - 1s - loss: 0.4604 - label_education_loss: 0.3715 - label_marital_loss: 0.0871 - label_education_auc: 0.8806 - label_marital_auc_1: 0.9949 - val_loss: 0.4737 - val_label_education_loss: 0.3802 - val_label_marital_loss: 0.0917 - val_label_education_auc: 0.8733 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 00033: early stopping\n",
      "test inceducationome AUC 0.8745\n",
      "test marital AUC 0.9945\n"
     ]
    }
   ],
   "source": [
    "#Test Shared_Bottom Model\n",
    "from shared_bottom import Shared_Bottom\n",
    "\n",
    "model = Shared_Bottom(dnn_feature_columns, num_tasks=2, task_types= ['binary', 'binary'], task_names=['label_education','label_marital'], bottom_dnn_units=[16], tower_dnn_units_lists=[[8],[8]])\n",
    "\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0, patience=10, verbose=1,\n",
    "                                       mode='min',baseline=None,restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n",
    "history = model.fit(train_model_input, [train['label_education'].values, train['label_marital'].values],batch_size=1024, epochs=100, verbose=2,validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "pred_ans = model.predict(test_model_input, batch_size=1024)\n",
    "\n",
    "print(\"test education AUC\", round(roc_auc_score(test['label_education'], pred_ans[0]), 4))\n",
    "print(\"test marital AUC\", round(roc_auc_score(test['label_marital'], pred_ans[1]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "195/195 - 2s - loss: 0.7163 - label_marital_loss: 0.2452 - label_education_loss: 0.4707 - label_marital_auc: 0.9735 - label_education_auc_1: 0.8243 - val_loss: 0.6675 - val_label_marital_loss: 0.2073 - val_label_education_loss: 0.4596 - val_label_marital_auc: 0.9805 - val_label_education_auc_1: 0.8511\n",
      "Epoch 2/100\n",
      "195/195 - 1s - loss: 0.6627 - label_marital_loss: 0.2101 - label_education_loss: 0.4519 - label_marital_auc: 0.9825 - label_education_auc_1: 0.8526 - val_loss: 0.6612 - val_label_marital_loss: 0.2185 - val_label_education_loss: 0.4420 - val_label_marital_auc: 0.9833 - val_label_education_auc_1: 0.8568\n",
      "Epoch 3/100\n",
      "195/195 - 1s - loss: 0.6567 - label_marital_loss: 0.2062 - label_education_loss: 0.4497 - label_marital_auc: 0.9833 - label_education_auc_1: 0.8562 - val_loss: 0.6606 - val_label_marital_loss: 0.2140 - val_label_education_loss: 0.4457 - val_label_marital_auc: 0.9825 - val_label_education_auc_1: 0.8621\n",
      "Epoch 4/100\n",
      "195/195 - 1s - loss: 0.6546 - label_marital_loss: 0.2047 - label_education_loss: 0.4491 - label_marital_auc: 0.9835 - label_education_auc_1: 0.8574 - val_loss: 0.6552 - val_label_marital_loss: 0.2091 - val_label_education_loss: 0.4452 - val_label_marital_auc: 0.9819 - val_label_education_auc_1: 0.8545\n",
      "Epoch 5/100\n",
      "195/195 - 1s - loss: 0.6537 - label_marital_loss: 0.2041 - label_education_loss: 0.4487 - label_marital_auc: 0.9835 - label_education_auc_1: 0.8579 - val_loss: 0.6539 - val_label_marital_loss: 0.1985 - val_label_education_loss: 0.4544 - val_label_marital_auc: 0.9843 - val_label_education_auc_1: 0.8576\n",
      "Epoch 6/100\n",
      "195/195 - 1s - loss: 0.6510 - label_marital_loss: 0.2030 - label_education_loss: 0.4470 - label_marital_auc: 0.9839 - label_education_auc_1: 0.8598 - val_loss: 0.6529 - val_label_marital_loss: 0.2004 - val_label_education_loss: 0.4515 - val_label_marital_auc: 0.9829 - val_label_education_auc_1: 0.8587\n",
      "Epoch 7/100\n",
      "195/195 - 1s - loss: 0.6501 - label_marital_loss: 0.2027 - label_education_loss: 0.4463 - label_marital_auc: 0.9837 - label_education_auc_1: 0.8606 - val_loss: 0.6533 - val_label_marital_loss: 0.2056 - val_label_education_loss: 0.4466 - val_label_marital_auc: 0.9841 - val_label_education_auc_1: 0.8598\n",
      "Epoch 8/100\n",
      "195/195 - 1s - loss: 0.6491 - label_marital_loss: 0.2023 - label_education_loss: 0.4457 - label_marital_auc: 0.9838 - label_education_auc_1: 0.8610 - val_loss: 0.6527 - val_label_marital_loss: 0.2051 - val_label_education_loss: 0.4465 - val_label_marital_auc: 0.9833 - val_label_education_auc_1: 0.8622\n",
      "Epoch 9/100\n",
      "195/195 - 1s - loss: 0.6485 - label_marital_loss: 0.2019 - label_education_loss: 0.4455 - label_marital_auc: 0.9837 - label_education_auc_1: 0.8612 - val_loss: 0.6519 - val_label_marital_loss: 0.2005 - val_label_education_loss: 0.4502 - val_label_marital_auc: 0.9822 - val_label_education_auc_1: 0.8588\n",
      "Epoch 10/100\n",
      "195/195 - 1s - loss: 0.6473 - label_marital_loss: 0.2014 - label_education_loss: 0.4447 - label_marital_auc: 0.9838 - label_education_auc_1: 0.8623 - val_loss: 0.6514 - val_label_marital_loss: 0.2020 - val_label_education_loss: 0.4483 - val_label_marital_auc: 0.9832 - val_label_education_auc_1: 0.8593\n",
      "Epoch 11/100\n",
      "195/195 - 1s - loss: 0.6473 - label_marital_loss: 0.2013 - label_education_loss: 0.4448 - label_marital_auc: 0.9837 - label_education_auc_1: 0.8620 - val_loss: 0.6538 - val_label_marital_loss: 0.2091 - val_label_education_loss: 0.4435 - val_label_marital_auc: 0.9838 - val_label_education_auc_1: 0.8633\n",
      "Epoch 12/100\n",
      "195/195 - 1s - loss: 0.6475 - label_marital_loss: 0.2016 - label_education_loss: 0.4446 - label_marital_auc: 0.9835 - label_education_auc_1: 0.8620 - val_loss: 0.6517 - val_label_marital_loss: 0.2100 - val_label_education_loss: 0.4404 - val_label_marital_auc: 0.9819 - val_label_education_auc_1: 0.8594\n",
      "Epoch 13/100\n",
      "195/195 - 1s - loss: 0.6468 - label_marital_loss: 0.2014 - label_education_loss: 0.4441 - label_marital_auc: 0.9837 - label_education_auc_1: 0.8627 - val_loss: 0.6511 - val_label_marital_loss: 0.2040 - val_label_education_loss: 0.4458 - val_label_marital_auc: 0.9839 - val_label_education_auc_1: 0.8581\n",
      "Epoch 14/100\n",
      "195/195 - 1s - loss: 0.6463 - label_marital_loss: 0.2013 - label_education_loss: 0.4437 - label_marital_auc: 0.9836 - label_education_auc_1: 0.8630 - val_loss: 0.6513 - val_label_marital_loss: 0.2071 - val_label_education_loss: 0.4429 - val_label_marital_auc: 0.9813 - val_label_education_auc_1: 0.8579\n",
      "Epoch 15/100\n",
      "195/195 - 1s - loss: 0.6462 - label_marital_loss: 0.2010 - label_education_loss: 0.4438 - label_marital_auc: 0.9836 - label_education_auc_1: 0.8629 - val_loss: 0.6510 - val_label_marital_loss: 0.2000 - val_label_education_loss: 0.4498 - val_label_marital_auc: 0.9828 - val_label_education_auc_1: 0.8549\n",
      "Epoch 16/100\n",
      "195/195 - 1s - loss: 0.6459 - label_marital_loss: 0.2012 - label_education_loss: 0.4434 - label_marital_auc: 0.9834 - label_education_auc_1: 0.8631 - val_loss: 0.6504 - val_label_marital_loss: 0.2034 - val_label_education_loss: 0.4457 - val_label_marital_auc: 0.9814 - val_label_education_auc_1: 0.8606\n",
      "Epoch 17/100\n",
      "195/195 - 1s - loss: 0.6452 - label_marital_loss: 0.2006 - label_education_loss: 0.4432 - label_marital_auc: 0.9837 - label_education_auc_1: 0.8635 - val_loss: 0.6513 - val_label_marital_loss: 0.2087 - val_label_education_loss: 0.4412 - val_label_marital_auc: 0.9810 - val_label_education_auc_1: 0.8619\n",
      "Epoch 18/100\n",
      "195/195 - 1s - loss: 0.6453 - label_marital_loss: 0.2009 - label_education_loss: 0.4430 - label_marital_auc: 0.9835 - label_education_auc_1: 0.8637 - val_loss: 0.6509 - val_label_marital_loss: 0.2025 - val_label_education_loss: 0.4470 - val_label_marital_auc: 0.9829 - val_label_education_auc_1: 0.8610\n",
      "Epoch 19/100\n",
      "195/195 - 1s - loss: 0.6446 - label_marital_loss: 0.2005 - label_education_loss: 0.4426 - label_marital_auc: 0.9837 - label_education_auc_1: 0.8641 - val_loss: 0.6522 - val_label_marital_loss: 0.2069 - val_label_education_loss: 0.4438 - val_label_marital_auc: 0.9790 - val_label_education_auc_1: 0.8611\n",
      "Epoch 20/100\n",
      "195/195 - 1s - loss: 0.6454 - label_marital_loss: 0.2009 - label_education_loss: 0.4429 - label_marital_auc: 0.9833 - label_education_auc_1: 0.8636 - val_loss: 0.6540 - val_label_marital_loss: 0.1985 - val_label_education_loss: 0.4541 - val_label_marital_auc: 0.9844 - val_label_education_auc_1: 0.8624\n",
      "Epoch 21/100\n",
      "195/195 - 1s - loss: 0.6447 - label_marital_loss: 0.2006 - label_education_loss: 0.4427 - label_marital_auc: 0.9836 - label_education_auc_1: 0.8640 - val_loss: 0.6524 - val_label_marital_loss: 0.2077 - val_label_education_loss: 0.4432 - val_label_marital_auc: 0.9842 - val_label_education_auc_1: 0.8616\n",
      "Epoch 22/100\n",
      "195/195 - 1s - loss: 0.6445 - label_marital_loss: 0.2006 - label_education_loss: 0.4424 - label_marital_auc: 0.9834 - label_education_auc_1: 0.8643 - val_loss: 0.6510 - val_label_marital_loss: 0.2090 - val_label_education_loss: 0.4405 - val_label_marital_auc: 0.9836 - val_label_education_auc_1: 0.8589\n",
      "Epoch 23/100\n",
      "195/195 - 1s - loss: 0.6445 - label_marital_loss: 0.2005 - label_education_loss: 0.4425 - label_marital_auc: 0.9835 - label_education_auc_1: 0.8641 - val_loss: 0.6517 - val_label_marital_loss: 0.2070 - val_label_education_loss: 0.4432 - val_label_marital_auc: 0.9828 - val_label_education_auc_1: 0.8554\n",
      "Epoch 24/100\n",
      "195/195 - 1s - loss: 0.6443 - label_marital_loss: 0.2003 - label_education_loss: 0.4424 - label_marital_auc: 0.9835 - label_education_auc_1: 0.8641 - val_loss: 0.6531 - val_label_marital_loss: 0.2036 - val_label_education_loss: 0.4479 - val_label_marital_auc: 0.9847 - val_label_education_auc_1: 0.8616\n",
      "Epoch 25/100\n",
      "195/195 - 1s - loss: 0.6443 - label_marital_loss: 0.2008 - label_education_loss: 0.4419 - label_marital_auc: 0.9833 - label_education_auc_1: 0.8647 - val_loss: 0.6512 - val_label_marital_loss: 0.1997 - val_label_education_loss: 0.4499 - val_label_marital_auc: 0.9841 - val_label_education_auc_1: 0.8561\n",
      "Epoch 26/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "195/195 - 1s - loss: 0.6441 - label_marital_loss: 0.2004 - label_education_loss: 0.4421 - label_marital_auc: 0.9836 - label_education_auc_1: 0.8646 - val_loss: 0.6510 - val_label_marital_loss: 0.2037 - val_label_education_loss: 0.4458 - val_label_marital_auc: 0.9835 - val_label_education_auc_1: 0.8552\n",
      "Epoch 00026: early stopping\n",
      "test education AUC 0.982\n",
      "test income AUC 0.8601\n"
     ]
    }
   ],
   "source": [
    "#Test ESSM Model\n",
    "from essm import ESSM\n",
    "#take marital as ctr task, take income as ctcvr task\n",
    "model = ESSM(dnn_feature_columns, task_type='binary', task_names=['label_marital', 'label_education'],\n",
    "        tower_dnn_units_lists=[[8],[8]])\n",
    "\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0, patience=10, verbose=1,\n",
    "                                       mode='min',baseline=None,restore_best_weights=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n",
    "history = model.fit(train_model_input, [train['label_marital'].values, train['label_education'].values],batch_size=1024, epochs=100, verbose=2, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n",
    "pred_ans = model.predict(test_model_input, batch_size=1024)\n",
    "\n",
    "print(\"test education AUC\", round(roc_auc_score(test['label_marital'], pred_ans[0]), 4))\n",
    "print(\"test income AUC\", round(roc_auc_score(test['label_education'], pred_ans[1]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "195/195 - 3s - loss: 0.5466 - label_education_loss: 0.4058 - label_marital_loss: 0.1404 - label_education_auc: 0.8513 - label_marital_auc_1: 0.9872 - val_loss: 0.4881 - val_label_education_loss: 0.3849 - val_label_marital_loss: 0.1026 - val_label_education_auc: 0.8691 - val_label_marital_auc_1: 0.9938\n",
      "Epoch 2/100\n",
      "195/195 - 2s - loss: 0.4790 - label_education_loss: 0.3819 - label_marital_loss: 0.0964 - label_education_auc: 0.8722 - label_marital_auc_1: 0.9938 - val_loss: 0.4842 - val_label_education_loss: 0.3830 - val_label_marital_loss: 0.1004 - val_label_education_auc: 0.8713 - val_label_marital_auc_1: 0.9940\n",
      "Epoch 3/100\n",
      "195/195 - 2s - loss: 0.4747 - label_education_loss: 0.3800 - label_marital_loss: 0.0938 - label_education_auc: 0.8739 - label_marital_auc_1: 0.9941 - val_loss: 0.4779 - val_label_education_loss: 0.3836 - val_label_marital_loss: 0.0934 - val_label_education_auc: 0.8715 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 4/100\n",
      "195/195 - 2s - loss: 0.4711 - label_education_loss: 0.3783 - label_marital_loss: 0.0918 - label_education_auc: 0.8752 - label_marital_auc_1: 0.9944 - val_loss: 0.4753 - val_label_education_loss: 0.3822 - val_label_marital_loss: 0.0921 - val_label_education_auc: 0.8716 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 5/100\n",
      "195/195 - 2s - loss: 0.4698 - label_education_loss: 0.3776 - label_marital_loss: 0.0911 - label_education_auc: 0.8758 - label_marital_auc_1: 0.9945 - val_loss: 0.4817 - val_label_education_loss: 0.3858 - val_label_marital_loss: 0.0949 - val_label_education_auc: 0.8710 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 6/100\n",
      "195/195 - 2s - loss: 0.4680 - label_education_loss: 0.3766 - label_marital_loss: 0.0902 - label_education_auc: 0.8764 - label_marital_auc_1: 0.9946 - val_loss: 0.4743 - val_label_education_loss: 0.3814 - val_label_marital_loss: 0.0917 - val_label_education_auc: 0.8722 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 7/100\n",
      "195/195 - 2s - loss: 0.4662 - label_education_loss: 0.3756 - label_marital_loss: 0.0894 - label_education_auc: 0.8773 - label_marital_auc_1: 0.9946 - val_loss: 0.4760 - val_label_education_loss: 0.3818 - val_label_marital_loss: 0.0931 - val_label_education_auc: 0.8726 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 8/100\n",
      "195/195 - 2s - loss: 0.4656 - label_education_loss: 0.3755 - label_marital_loss: 0.0888 - label_education_auc: 0.8773 - label_marital_auc_1: 0.9947 - val_loss: 0.4747 - val_label_education_loss: 0.3819 - val_label_marital_loss: 0.0915 - val_label_education_auc: 0.8726 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 9/100\n",
      "195/195 - 2s - loss: 0.4637 - label_education_loss: 0.3742 - label_marital_loss: 0.0881 - label_education_auc: 0.8783 - label_marital_auc_1: 0.9948 - val_loss: 0.4738 - val_label_education_loss: 0.3809 - val_label_marital_loss: 0.0915 - val_label_education_auc: 0.8728 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 10/100\n",
      "195/195 - 2s - loss: 0.4624 - label_education_loss: 0.3733 - label_marital_loss: 0.0876 - label_education_auc: 0.8789 - label_marital_auc_1: 0.9949 - val_loss: 0.4737 - val_label_education_loss: 0.3809 - val_label_marital_loss: 0.0914 - val_label_education_auc: 0.8728 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 11/100\n",
      "195/195 - 2s - loss: 0.4605 - label_education_loss: 0.3718 - label_marital_loss: 0.0872 - label_education_auc: 0.8802 - label_marital_auc_1: 0.9949 - val_loss: 0.4743 - val_label_education_loss: 0.3811 - val_label_marital_loss: 0.0916 - val_label_education_auc: 0.8724 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 12/100\n",
      "195/195 - 2s - loss: 0.4598 - label_education_loss: 0.3714 - label_marital_loss: 0.0867 - label_education_auc: 0.8802 - label_marital_auc_1: 0.9950 - val_loss: 0.4750 - val_label_education_loss: 0.3821 - val_label_marital_loss: 0.0913 - val_label_education_auc: 0.8722 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 13/100\n",
      "195/195 - 2s - loss: 0.4579 - label_education_loss: 0.3700 - label_marital_loss: 0.0862 - label_education_auc: 0.8814 - label_marital_auc_1: 0.9950 - val_loss: 0.4754 - val_label_education_loss: 0.3819 - val_label_marital_loss: 0.0917 - val_label_education_auc: 0.8721 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 14/100\n",
      "195/195 - 2s - loss: 0.4573 - label_education_loss: 0.3696 - label_marital_loss: 0.0859 - label_education_auc: 0.8817 - label_marital_auc_1: 0.9951 - val_loss: 0.4770 - val_label_education_loss: 0.3831 - val_label_marital_loss: 0.0921 - val_label_education_auc: 0.8721 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 15/100\n",
      "195/195 - 2s - loss: 0.4556 - label_education_loss: 0.3684 - label_marital_loss: 0.0854 - label_education_auc: 0.8827 - label_marital_auc_1: 0.9951 - val_loss: 0.4773 - val_label_education_loss: 0.3824 - val_label_marital_loss: 0.0930 - val_label_education_auc: 0.8724 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 16/100\n",
      "195/195 - 2s - loss: 0.4538 - label_education_loss: 0.3670 - label_marital_loss: 0.0849 - label_education_auc: 0.8835 - label_marital_auc_1: 0.9952 - val_loss: 0.4767 - val_label_education_loss: 0.3829 - val_label_marital_loss: 0.0918 - val_label_education_auc: 0.8719 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 17/100\n",
      "195/195 - 2s - loss: 0.4527 - label_education_loss: 0.3665 - label_marital_loss: 0.0842 - label_education_auc: 0.8839 - label_marital_auc_1: 0.9952 - val_loss: 0.4790 - val_label_education_loss: 0.3828 - val_label_marital_loss: 0.0941 - val_label_education_auc: 0.8715 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 18/100\n",
      "195/195 - 2s - loss: 0.4515 - label_education_loss: 0.3652 - label_marital_loss: 0.0842 - label_education_auc: 0.8848 - label_marital_auc_1: 0.9953 - val_loss: 0.4831 - val_label_education_loss: 0.3868 - val_label_marital_loss: 0.0942 - val_label_education_auc: 0.8710 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 19/100\n",
      "195/195 - 2s - loss: 0.4502 - label_education_loss: 0.3643 - label_marital_loss: 0.0837 - label_education_auc: 0.8856 - label_marital_auc_1: 0.9953 - val_loss: 0.4798 - val_label_education_loss: 0.3846 - val_label_marital_loss: 0.0929 - val_label_education_auc: 0.8715 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 20/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "195/195 - 2s - loss: 0.4493 - label_education_loss: 0.3634 - label_marital_loss: 0.0837 - label_education_auc: 0.8861 - label_marital_auc_1: 0.9953 - val_loss: 0.4842 - val_label_education_loss: 0.3868 - val_label_marital_loss: 0.0951 - val_label_education_auc: 0.8706 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 00020: early stopping\n",
      "test education AUC 0.8734\n",
      "test marital AUC 0.9946\n"
     ]
    }
   ],
   "source": [
    "from mmoe import MMOE\n",
    "model = MMOE(dnn_feature_columns, num_tasks=2, task_types=['binary', 'binary'], task_names=['label_education','label_marital'], \n",
    "             num_experts=8, expert_dnn_units=[16], gate_dnn_units=None, tower_dnn_units_lists=[[8],[8]])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=[\"binary_crossentropy\", \"binary_crossentropy\"],metrics=['AUC'])\n",
    "\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0, patience=10, verbose=1,\n",
    "                                       mode='min',baseline=None,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_model_input, [train['label_education'].values, train['label_marital'].values], \n",
    "                    batch_size=1024, epochs=100, verbose=2, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n",
    "pred_ans = model.predict(test_model_input, batch_size=1024)\n",
    "print(\"test education AUC\", round(roc_auc_score(test['label_education'], pred_ans[0]), 4))\n",
    "print(\"test marital AUC\", round(roc_auc_score(test['label_marital'], pred_ans[1]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "195/195 - 3s - loss: 0.5478 - label_education_loss: 0.4071 - label_marital_loss: 0.1402 - label_education_auc: 0.8507 - label_marital_auc_1: 0.9874 - val_loss: 0.4842 - val_label_education_loss: 0.3847 - val_label_marital_loss: 0.0989 - val_label_education_auc: 0.8697 - val_label_marital_auc_1: 0.9938\n",
      "Epoch 2/100\n",
      "195/195 - 2s - loss: 0.4765 - label_education_loss: 0.3807 - label_marital_loss: 0.0950 - label_education_auc: 0.8735 - label_marital_auc_1: 0.9940 - val_loss: 0.4790 - val_label_education_loss: 0.3833 - val_label_marital_loss: 0.0949 - val_label_education_auc: 0.8709 - val_label_marital_auc_1: 0.9941\n",
      "Epoch 3/100\n",
      "195/195 - 2s - loss: 0.4733 - label_education_loss: 0.3790 - label_marital_loss: 0.0935 - label_education_auc: 0.8749 - label_marital_auc_1: 0.9942 - val_loss: 0.4772 - val_label_education_loss: 0.3822 - val_label_marital_loss: 0.0940 - val_label_education_auc: 0.8716 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 4/100\n",
      "195/195 - 2s - loss: 0.4717 - label_education_loss: 0.3779 - label_marital_loss: 0.0928 - label_education_auc: 0.8756 - label_marital_auc_1: 0.9943 - val_loss: 0.4768 - val_label_education_loss: 0.3826 - val_label_marital_loss: 0.0932 - val_label_education_auc: 0.8717 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 5/100\n",
      "195/195 - 2s - loss: 0.4687 - label_education_loss: 0.3767 - label_marital_loss: 0.0909 - label_education_auc: 0.8767 - label_marital_auc_1: 0.9945 - val_loss: 0.4751 - val_label_education_loss: 0.3808 - val_label_marital_loss: 0.0932 - val_label_education_auc: 0.8728 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 6/100\n",
      "195/195 - 2s - loss: 0.4662 - label_education_loss: 0.3752 - label_marital_loss: 0.0898 - label_education_auc: 0.8778 - label_marital_auc_1: 0.9946 - val_loss: 0.4740 - val_label_education_loss: 0.3805 - val_label_marital_loss: 0.0923 - val_label_education_auc: 0.8733 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 7/100\n",
      "195/195 - 2s - loss: 0.4647 - label_education_loss: 0.3743 - label_marital_loss: 0.0892 - label_education_auc: 0.8785 - label_marital_auc_1: 0.9947 - val_loss: 0.4740 - val_label_education_loss: 0.3816 - val_label_marital_loss: 0.0911 - val_label_education_auc: 0.8730 - val_label_marital_auc_1: 0.9945\n",
      "Epoch 8/100\n",
      "195/195 - 2s - loss: 0.4626 - label_education_loss: 0.3730 - label_marital_loss: 0.0883 - label_education_auc: 0.8795 - label_marital_auc_1: 0.9948 - val_loss: 0.4740 - val_label_education_loss: 0.3806 - val_label_marital_loss: 0.0920 - val_label_education_auc: 0.8728 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 9/100\n",
      "195/195 - 2s - loss: 0.4610 - label_education_loss: 0.3718 - label_marital_loss: 0.0878 - label_education_auc: 0.8804 - label_marital_auc_1: 0.9948 - val_loss: 0.4759 - val_label_education_loss: 0.3817 - val_label_marital_loss: 0.0927 - val_label_education_auc: 0.8725 - val_label_marital_auc_1: 0.9941\n",
      "Epoch 10/100\n",
      "195/195 - 2s - loss: 0.4588 - label_education_loss: 0.3703 - label_marital_loss: 0.0869 - label_education_auc: 0.8815 - label_marital_auc_1: 0.9949 - val_loss: 0.4751 - val_label_education_loss: 0.3816 - val_label_marital_loss: 0.0920 - val_label_education_auc: 0.8719 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 11/100\n",
      "195/195 - 2s - loss: 0.4576 - label_education_loss: 0.3696 - label_marital_loss: 0.0864 - label_education_auc: 0.8821 - label_marital_auc_1: 0.9950 - val_loss: 0.4794 - val_label_education_loss: 0.3818 - val_label_marital_loss: 0.0960 - val_label_education_auc: 0.8715 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 12/100\n",
      "195/195 - 2s - loss: 0.4559 - label_education_loss: 0.3684 - label_marital_loss: 0.0858 - label_education_auc: 0.8829 - label_marital_auc_1: 0.9951 - val_loss: 0.4771 - val_label_education_loss: 0.3826 - val_label_marital_loss: 0.0926 - val_label_education_auc: 0.8716 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 13/100\n",
      "195/195 - 2s - loss: 0.4541 - label_education_loss: 0.3668 - label_marital_loss: 0.0854 - label_education_auc: 0.8840 - label_marital_auc_1: 0.9951 - val_loss: 0.4800 - val_label_education_loss: 0.3841 - val_label_marital_loss: 0.0940 - val_label_education_auc: 0.8711 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 14/100\n",
      "195/195 - 2s - loss: 0.4520 - label_education_loss: 0.3656 - label_marital_loss: 0.0845 - label_education_auc: 0.8848 - label_marital_auc_1: 0.9952 - val_loss: 0.4791 - val_label_education_loss: 0.3832 - val_label_marital_loss: 0.0939 - val_label_education_auc: 0.8712 - val_label_marital_auc_1: 0.9940\n",
      "Epoch 15/100\n",
      "195/195 - 2s - loss: 0.4499 - label_education_loss: 0.3642 - label_marital_loss: 0.0836 - label_education_auc: 0.8859 - label_marital_auc_1: 0.9953 - val_loss: 0.4813 - val_label_education_loss: 0.3843 - val_label_marital_loss: 0.0949 - val_label_education_auc: 0.8710 - val_label_marital_auc_1: 0.9940\n",
      "Epoch 16/100\n",
      "195/195 - 2s - loss: 0.4494 - label_education_loss: 0.3635 - label_marital_loss: 0.0837 - label_education_auc: 0.8862 - label_marital_auc_1: 0.9953 - val_loss: 0.4822 - val_label_education_loss: 0.3844 - val_label_marital_loss: 0.0956 - val_label_education_auc: 0.8715 - val_label_marital_auc_1: 0.9940\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "195/195 - 2s - loss: 0.4469 - label_education_loss: 0.3618 - label_marital_loss: 0.0829 - label_education_auc: 0.8877 - label_marital_auc_1: 0.9954 - val_loss: 0.4854 - val_label_education_loss: 0.3860 - val_label_marital_loss: 0.0971 - val_label_education_auc: 0.8694 - val_label_marital_auc_1: 0.9939\n",
      "Epoch 00017: early stopping\n",
      "test education AUC 0.8736\n",
      "test marital AUC 0.9946\n"
     ]
    }
   ],
   "source": [
    "from ple_cgc import PLE_CGC\n",
    "\n",
    "model = PLE_CGC(dnn_feature_columns, num_tasks=2, task_types=['binary', 'binary'], task_names=['label_education','label_marital'], \n",
    "                num_experts_specific=4, num_experts_shared=4, expert_dnn_units=[16], gate_dnn_units=None, tower_dnn_units_lists=[[8],[8]])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n",
    "\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=10, verbose=1,\n",
    "                                       mode='min',baseline=None,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_model_input, [train['label_education'].values, train['label_marital'].values], \n",
    "                    batch_size=1024, epochs=100, verbose=2, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n",
    "pred_ans = model.predict(test_model_input, batch_size=1024)\n",
    "print(\"test education AUC\", round(roc_auc_score(test['label_education'], pred_ans[0]), 4))\n",
    "print(\"test marital AUC\", round(roc_auc_score(test['label_marital'], pred_ans[1]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "195/195 - 4s - loss: 0.8963 - label_education_loss: 0.4108 - label_marital_loss: 0.4851 - label_education_auc: 0.8497 - label_marital_auc_1: 0.8235 - val_loss: 0.5634 - val_label_education_loss: 0.3849 - val_label_marital_loss: 0.1778 - val_label_education_auc: 0.8693 - val_label_marital_auc_1: 0.9852\n",
      "Epoch 2/100\n",
      "195/195 - 3s - loss: 0.5222 - label_education_loss: 0.3811 - label_marital_loss: 0.1403 - label_education_auc: 0.8728 - label_marital_auc_1: 0.9881 - val_loss: 0.5060 - val_label_education_loss: 0.3831 - val_label_marital_loss: 0.1221 - val_label_education_auc: 0.8707 - val_label_marital_auc_1: 0.9903\n",
      "Epoch 3/100\n",
      "195/195 - 3s - loss: 0.4935 - label_education_loss: 0.3795 - label_marital_loss: 0.1131 - label_education_auc: 0.8743 - label_marital_auc_1: 0.9927 - val_loss: 0.4915 - val_label_education_loss: 0.3826 - val_label_marital_loss: 0.1080 - val_label_education_auc: 0.8716 - val_label_marital_auc_1: 0.9939\n",
      "Epoch 4/100\n",
      "195/195 - 3s - loss: 0.4820 - label_education_loss: 0.3774 - label_marital_loss: 0.1036 - label_education_auc: 0.8760 - label_marital_auc_1: 0.9941 - val_loss: 0.4855 - val_label_education_loss: 0.3823 - val_label_marital_loss: 0.1022 - val_label_education_auc: 0.8729 - val_label_marital_auc_1: 0.9941\n",
      "Epoch 5/100\n",
      "195/195 - 3s - loss: 0.4761 - label_education_loss: 0.3765 - label_marital_loss: 0.0985 - label_education_auc: 0.8767 - label_marital_auc_1: 0.9944 - val_loss: 0.4827 - val_label_education_loss: 0.3819 - val_label_marital_loss: 0.0996 - val_label_education_auc: 0.8732 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 6/100\n",
      "195/195 - 5s - loss: 0.4719 - label_education_loss: 0.3753 - label_marital_loss: 0.0954 - label_education_auc: 0.8776 - label_marital_auc_1: 0.9946 - val_loss: 0.4799 - val_label_education_loss: 0.3809 - val_label_marital_loss: 0.0978 - val_label_education_auc: 0.8729 - val_label_marital_auc_1: 0.9944\n",
      "Epoch 7/100\n",
      "195/195 - 3s - loss: 0.4675 - label_education_loss: 0.3733 - label_marital_loss: 0.0929 - label_education_auc: 0.8789 - label_marital_auc_1: 0.9947 - val_loss: 0.4811 - val_label_education_loss: 0.3818 - val_label_marital_loss: 0.0980 - val_label_education_auc: 0.8731 - val_label_marital_auc_1: 0.9941\n",
      "Epoch 8/100\n",
      "195/195 - 3s - loss: 0.4660 - label_education_loss: 0.3725 - label_marital_loss: 0.0921 - label_education_auc: 0.8797 - label_marital_auc_1: 0.9947 - val_loss: 0.4773 - val_label_education_loss: 0.3806 - val_label_marital_loss: 0.0952 - val_label_education_auc: 0.8732 - val_label_marital_auc_1: 0.9943\n",
      "Epoch 9/100\n",
      "195/195 - 3s - loss: 0.4631 - label_education_loss: 0.3711 - label_marital_loss: 0.0905 - label_education_auc: 0.8806 - label_marital_auc_1: 0.9948 - val_loss: 0.4791 - val_label_education_loss: 0.3820 - val_label_marital_loss: 0.0955 - val_label_education_auc: 0.8729 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 10/100\n",
      "195/195 - 3s - loss: 0.4610 - label_education_loss: 0.3695 - label_marital_loss: 0.0898 - label_education_auc: 0.8817 - label_marital_auc_1: 0.9948 - val_loss: 0.4791 - val_label_education_loss: 0.3827 - val_label_marital_loss: 0.0948 - val_label_education_auc: 0.8720 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 11/100\n",
      "195/195 - 3s - loss: 0.4581 - label_education_loss: 0.3678 - label_marital_loss: 0.0886 - label_education_auc: 0.8830 - label_marital_auc_1: 0.9950 - val_loss: 0.4802 - val_label_education_loss: 0.3829 - val_label_marital_loss: 0.0955 - val_label_education_auc: 0.8724 - val_label_marital_auc_1: 0.9942\n",
      "Epoch 12/100\n",
      "195/195 - 3s - loss: 0.4554 - label_education_loss: 0.3660 - label_marital_loss: 0.0875 - label_education_auc: 0.8843 - label_marital_auc_1: 0.9951 - val_loss: 0.4831 - val_label_education_loss: 0.3852 - val_label_marital_loss: 0.0960 - val_label_education_auc: 0.8717 - val_label_marital_auc_1: 0.9940\n",
      "Epoch 13/100\n",
      "195/195 - 3s - loss: 0.4519 - label_education_loss: 0.3644 - label_marital_loss: 0.0855 - label_education_auc: 0.8853 - label_marital_auc_1: 0.9951 - val_loss: 0.4829 - val_label_education_loss: 0.3849 - val_label_marital_loss: 0.0960 - val_label_education_auc: 0.8705 - val_label_marital_auc_1: 0.9938\n",
      "Epoch 14/100\n",
      "195/195 - 3s - loss: 0.4497 - label_education_loss: 0.3626 - label_marital_loss: 0.0850 - label_education_auc: 0.8867 - label_marital_auc_1: 0.9951 - val_loss: 0.4853 - val_label_education_loss: 0.3879 - val_label_marital_loss: 0.0952 - val_label_education_auc: 0.8692 - val_label_marital_auc_1: 0.9939\n",
      "Epoch 15/100\n",
      "195/195 - 3s - loss: 0.4474 - label_education_loss: 0.3612 - label_marital_loss: 0.0840 - label_education_auc: 0.8875 - label_marital_auc_1: 0.9953 - val_loss: 0.4855 - val_label_education_loss: 0.3870 - val_label_marital_loss: 0.0963 - val_label_education_auc: 0.8711 - val_label_marital_auc_1: 0.9937\n",
      "Epoch 16/100\n",
      "195/195 - 3s - loss: 0.4448 - label_education_loss: 0.3591 - label_marital_loss: 0.0834 - label_education_auc: 0.8890 - label_marital_auc_1: 0.9953 - val_loss: 0.4877 - val_label_education_loss: 0.3892 - val_label_marital_loss: 0.0961 - val_label_education_auc: 0.8677 - val_label_marital_auc_1: 0.9939\n",
      "Epoch 17/100\n",
      "195/195 - 3s - loss: 0.4427 - label_education_loss: 0.3574 - label_marital_loss: 0.0829 - label_education_auc: 0.8901 - label_marital_auc_1: 0.9954 - val_loss: 0.4894 - val_label_education_loss: 0.3898 - val_label_marital_loss: 0.0972 - val_label_education_auc: 0.8676 - val_label_marital_auc_1: 0.9937\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "195/195 - 3s - loss: 0.4400 - label_education_loss: 0.3555 - label_marital_loss: 0.0820 - label_education_auc: 0.8916 - label_marital_auc_1: 0.9955 - val_loss: 0.4951 - val_label_education_loss: 0.3952 - val_label_marital_loss: 0.0974 - val_label_education_auc: 0.8669 - val_label_marital_auc_1: 0.9936\n",
      "Epoch 00018: early stopping\n",
      "test education AUC 0.8737\n",
      "test marital AUC 0.9945\n"
     ]
    }
   ],
   "source": [
    "from ple import PLE\n",
    "\n",
    "model = PLE(dnn_feature_columns, num_tasks=2, task_types=['binary', 'binary'], task_names=['label_education','label_marital'], \n",
    "            num_levels=2, num_experts_specific=4, num_experts_shared=4, expert_dnn_units=[16],  gate_dnn_units=None,                                 tower_dnn_units_lists=[[8],[8]])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n",
    "\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=10, verbose=1,\n",
    "                                       mode='min',baseline=None,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_model_input, [train['label_education'].values, train['label_marital'].values], \n",
    "                    batch_size=1024, epochs=100, verbose=2, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n",
    "pred_ans = model.predict(test_model_input, batch_size=1024)\n",
    "print(\"test education AUC\", round(roc_auc_score(test['label_education'], pred_ans[0]), 4))2\n",
    "print(\"test marital AUC\", round(roc_auc_score(test['label_marital'], pred_ans[1]), 4))"
   ]
  }
 ]
}